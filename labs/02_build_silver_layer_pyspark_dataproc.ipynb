{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fc7e250",
   "metadata": {},
   "source": [
    "# 02 — Build Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb2179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession, functions as F, Window\n",
    "spark = SparkSession.builder.appName(\"silver-dataproc\").getOrCreate()\n",
    "GCS_BASE  = os.getenv(\"GCS_BASE\", \"/user/nb\")\n",
    "print(\"GCS_BASE:\", GCS_BASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7998f12",
   "metadata": {},
   "source": [
    "### Table: `silver.crm_cust_info`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87861149",
   "metadata": {},
   "source": [
    "**Changes & Why**\n",
    "- Trim names; standardize marital status & gender.\n",
    "- Deduplicate by latest `cst_create_date` (SCD‑1).\n",
    "- Add audit timestamp.\n",
    "\n",
    "**Columns (post‑transform):**\n",
    "\n",
    "| Column | Type | Nullable | Description |\n",
    "|---|---|---|---|\n",
    "| `cst_id` | `int` | Yes | Natural key; dedup by latest create_date |\n",
    "| `cst_key` | `string` | Yes | Business key to map ERP |\n",
    "| `cst_firstname` | `string` | Yes | Trimmed |\n",
    "| `cst_lastname` | `string` | Yes | Trimmed |\n",
    "| `cst_marital_status` | `string` | Yes | Standardized: Single/Married/n/a |\n",
    "| `cst_gndr` | `string` | Yes | Standardized: Male/Female/n/a |\n",
    "| `cst_create_date` | `date` | Yes | Original create date |\n",
    "| `dwh_create_date` | `timestamp` | Yes | Audit timestamp |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072eae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, Window\n",
    "\n",
    "# Load\n",
    "b = spark.read.parquet(f\"{GCS_BASE}/bronze/crm_cust_info\")\n",
    "\n",
    "# Normalize IDs/names first and remove null/blank IDs\n",
    "clean = (\n",
    "    b.withColumn(\"cst_id\", F.trim(F.col(\"cst_id\")).cast(\"string\"))\n",
    "     .withColumn(\"cst_firstname\", F.trim(F.col(\"cst_firstname\")))\n",
    "     .withColumn(\"cst_lastname\",  F.trim(F.col(\"cst_lastname\")))\n",
    "     .filter(F.col(\"cst_id\").isNotNull() & (F.col(\"cst_id\") != \"\"))   # <-- drop NULL/blank IDs\n",
    ")\n",
    "\n",
    "# Dedupe by most recent create_date per cst_id\n",
    "w = Window.partitionBy(\"cst_id\").orderBy(F.col(\"cst_create_date\").desc_nulls_last())\n",
    "dedup = clean.withColumn(\"rn\", F.row_number().over(w)).filter(F.col(\"rn\") == 1).drop(\"rn\")\n",
    "\n",
    "# Standardize attributes\n",
    "out = (\n",
    "    dedup\n",
    "    .withColumn(\n",
    "        \"cst_marital_status\",\n",
    "        F.when(F.upper(F.trim(F.col(\"cst_marital_status\"))).isin(\"S\", \"SINGLE\"), \"Single\")\n",
    "         .when(F.upper(F.trim(F.col(\"cst_marital_status\"))).isin(\"M\", \"MARRIED\"), \"Married\")\n",
    "         .otherwise(F.lit(\"n/a\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"cst_gndr\",\n",
    "        F.when(F.upper(F.trim(F.col(\"cst_gndr\"))).isin(\"F\", \"FEMALE\"), \"Female\")\n",
    "         .when(F.upper(F.trim(F.col(\"cst_gndr\"))).isin(\"M\", \"MALE\"), \"Male\")\n",
    "         .otherwise(F.lit(\"n/a\"))\n",
    "    )\n",
    "    .withColumn(\"dwh_create_date\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "# Write\n",
    "out_path = f\"{GCS_BASE}/silver/crm_cust_info\"\n",
    "out.write.mode(\"overwrite\").parquet(out_path)\n",
    "\n",
    "# Quick check\n",
    "spark.read.parquet(out_path).createOrReplaceTempView(\"silver_crm_cust_info\")\n",
    "print(\"Rows:\", spark.read.parquet(out_path).count())\n",
    "spark.sql(\"SELECT * FROM silver_crm_cust_info LIMIT 10\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efba08c",
   "metadata": {},
   "source": [
    "### Table: `silver.crm_prd_info`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d3575a",
   "metadata": {},
   "source": [
    "**Changes & Why**\n",
    "- Derive `cat_id`; decode `prd_line`.\n",
    "- Effective dating via LEAD-1.\n",
    "- Add audit timestamp.\n",
    "\n",
    "**Columns (post‑transform):**\n",
    "\n",
    "| Column | Type | Nullable | Description |\n",
    "|---|---|---|---|\n",
    "| `prd_id` | `int` | Yes | Natural product id |\n",
    "| `cat_id` | `string` | Yes | Derived from prd_key |\n",
    "| `prd_key` | `string` | Yes | Clean business key |\n",
    "| `prd_nm` | `string` | Yes | Product name |\n",
    "| `prd_cost` | `int` | Yes | Unit cost |\n",
    "| `prd_line` | `string` | Yes | Decoded label |\n",
    "| `prd_start_dt` | `date` | Yes | Start date |\n",
    "| `prd_end_dt` | `date` | Yes | Effective end date via LEAD-1 |\n",
    "| `dwh_create_date` | `timestamp` | Yes | Audit timestamp |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1885b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = spark.read.parquet(f\"{GCS_BASE}/bronze/crm_prd_info\")\n",
    "tmp = (b.withColumn(\"cat_id\", F.regexp_replace(F.substring(\"prd_key\",1,5), \"-\", \"_\"))\n",
    "         .withColumn(\"prd_key_clean\", F.substring(\"prd_key\",7,100)))\n",
    "w = Window.partitionBy(\"prd_key_clean\").orderBy(F.col(\"prd_start_dt\").asc_nulls_last())\n",
    "out = (tmp\n",
    "    .withColumn(\"prd_line\", F.when(F.upper(F.trim(\"prd_line\"))==\"M\",\"Mountain\")\n",
    "                             .when(F.upper(F.trim(\"prd_line\"))==\"R\",\"Road\")\n",
    "                             .when(F.upper(F.trim(\"prd_line\"))==\"S\",\"Other Sales\")\n",
    "                             .when(F.upper(F.trim(\"prd_line\"))==\"T\",\"Touring\")\n",
    "                             .otherwise(F.lit(\"n/a\")))\n",
    "    .withColumn(\"prd_start_dt\", F.to_date(\"prd_start_dt\"))\n",
    "    .withColumn(\"prd_end_dt\", F.to_date(F.lead(\"prd_start_dt\").over(w) - F.expr(\"INTERVAL 1 DAY\")))\n",
    "    .withColumn(\"prd_key\", F.col(\"prd_key_clean\")).drop(\"prd_key_clean\")\n",
    "    .withColumn(\"dwh_create_date\", F.current_timestamp())\n",
    ")\n",
    "out_path = f\"{GCS_BASE}/silver/crm_prd_info\"\n",
    "out.write.mode(\"overwrite\").parquet(out_path)\n",
    "spark.read.parquet(out_path).createOrReplaceTempView(\"silver_crm_prd_info\")\n",
    "print(\"Rows:\", spark.read.parquet(out_path).count())\n",
    "spark.sql(\"SELECT * FROM silver_crm_prd_info LIMIT 10\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fb9ff5",
   "metadata": {},
   "source": [
    "### Table: `silver.crm_sales_details`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743995c8",
   "metadata": {},
   "source": [
    "**Changes & Why**\n",
    "- Parse dates; recompute sales if inconsistent; backfill price; add audit timestamp.\n",
    "\n",
    "**Columns (post‑transform):**\n",
    "\n",
    "| Column | Type | Nullable | Description |\n",
    "|---|---|---|---|\n",
    "| `sls_ord_num` | `string` | Yes | Order number |\n",
    "| `sls_prd_key` | `string` | Yes | Product key |\n",
    "| `sls_cust_id` | `int` | Yes | Customer id |\n",
    "| `sls_order_dt` | `date` | Yes | Parsed from YYYYMMDD |\n",
    "| `sls_ship_dt` | `date` | Yes | Parsed from YYYYMMDD |\n",
    "| `sls_due_dt` | `date` | Yes | Parsed from YYYYMMDD |\n",
    "| `sls_sales` | `int` | Yes | Recomputed if inconsistent |\n",
    "| `sls_quantity` | `int` | Yes | Units |\n",
    "| `sls_price` | `int` | Yes | Backfilled if missing |\n",
    "| `dwh_create_date` | `timestamp` | Yes | Audit timestamp |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ee1f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "b = spark.read.parquet(f\"{GCS_BASE}/bronze/crm_sales_details\")\n",
    "\n",
    "def parse_yyyymmdd(col):\n",
    "    # Input can be int or string; cast to string before parsing\n",
    "    return F.to_date(F.col(col).cast(\"string\"), \"yyyyMMdd\")\n",
    "\n",
    "# Normalize numeric columns to doubles once to avoid type issues\n",
    "qty   = F.col(\"sls_quantity\").cast(\"double\")\n",
    "price = F.col(\"sls_price\").cast(\"double\")\n",
    "sales = F.col(\"sls_sales\").cast(\"double\")\n",
    "\n",
    "# Useful helpers\n",
    "len_is_8 = lambda c: F.length(F.col(c).cast(\"string\")) == 8\n",
    "is_zero  = lambda c: F.col(c) == F.lit(0)\n",
    "\n",
    "out = (\n",
    "    b\n",
    "    # Clean dates: set to null if 0 or malformed, else parse yyyyMMdd\n",
    "    .withColumn(\n",
    "        \"sls_order_dt\",\n",
    "        F.when(is_zero(\"sls_order_dt\") | (~len_is_8(\"sls_order_dt\")), F.lit(None))\n",
    "         .otherwise(parse_yyyymmdd(\"sls_order_dt\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"sls_ship_dt\",\n",
    "        F.when(is_zero(\"sls_ship_dt\") | (~len_is_8(\"sls_ship_dt\")), F.lit(None))\n",
    "         .otherwise(parse_yyyymmdd(\"sls_ship_dt\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"sls_due_dt\",\n",
    "        F.when(is_zero(\"sls_due_dt\") | (~len_is_8(\"sls_due_dt\")), F.lit(None))\n",
    "         .otherwise(parse_yyyymmdd(\"sls_due_dt\"))\n",
    "    )\n",
    "\n",
    "    # Recompute sales if missing/invalid\n",
    "    .withColumn(\"sls_sales_fix\", qty * F.abs(price))\n",
    "    .withColumn(\n",
    "        \"sls_sales\",\n",
    "        F.when(sales.isNull() | (sales <= 0) | (sales != F.col(\"sls_sales_fix\")),\n",
    "               F.col(\"sls_sales_fix\")\n",
    "        ).otherwise(sales)\n",
    "    )\n",
    "\n",
    "    # Recompute price if missing/invalid: sales / qty (qty==0 -> null)\n",
    "    .withColumn(\n",
    "        \"sls_price\",\n",
    "        F.when(price.isNull() | (price <= 0),\n",
    "               (F.col(\"sls_sales\") / F.nullif(qty, F.lit(0.0))).cast(\"double\")\n",
    "        ).otherwise(price)\n",
    "    )\n",
    "\n",
    "    .drop(\"sls_sales_fix\")\n",
    "    .withColumn(\"dwh_create_date\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "out_path = f\"{GCS_BASE}/silver/crm_sales_details\"\n",
    "out.write.mode(\"overwrite\").parquet(out_path)\n",
    "\n",
    "spark.read.parquet(out_path).createOrReplaceTempView(\"silver_crm_sales_details\")\n",
    "print(\"Rows:\", spark.read.parquet(out_path).count())\n",
    "spark.sql(\"SELECT * FROM silver_crm_sales_details LIMIT 10\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefc2400",
   "metadata": {},
   "source": [
    "### Table: `silver.erp_cust_az12`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617664d2",
   "metadata": {},
   "source": [
    "**Changes & Why**\n",
    "- Strip `NAS` in `cid`; future `bdate` → NULL; normalize gender; add audit timestamp.\n",
    "\n",
    "**Columns (post‑transform):**\n",
    "\n",
    "| Column | Type | Nullable | Description |\n",
    "|---|---|---|---|\n",
    "| `cid` | `string` | Yes | NAS prefix stripped where present |\n",
    "| `bdate` | `date` | Yes | Future dates set to NULL |\n",
    "| `gen` | `string` | Yes | Normalized gender |\n",
    "| `dwh_create_date` | `timestamp` | Yes | Audit timestamp |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b967ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "b = spark.read.parquet(f\"{GCS_BASE}/bronze/erp_cust_az12\")\n",
    "out = (b\n",
    "    .withColumn(\"cid\", F.when(F.col(\"cid\").startswith(\"NAS\"), F.expr(\"substring(cid,4)\")).otherwise(F.col(\"cid\")))\n",
    "    .withColumn(\"bdate\", F.when(F.col(\"bdate\") > F.current_date(), F.lit(None)).otherwise(F.col(\"bdate\")))\n",
    "    .withColumn(\"gen\", F.when(F.upper(F.trim(\"gen\")).isin(\"F\",\"FEMALE\"), F.lit(\"Female\"))\n",
    "                        .when(F.upper(F.trim(\"gen\")).isin(\"M\",\"MALE\"), F.lit(\"Male\"))\n",
    "                        .otherwise(F.lit(\"n/a\")))\n",
    "    .withColumn(\"dwh_create_date\", F.current_timestamp())\n",
    ")\n",
    "out_path = f\"{GCS_BASE}/silver/erp_cust_az12\"\n",
    "out.write.mode(\"overwrite\").parquet(out_path)\n",
    "spark.read.parquet(out_path).createOrReplaceTempView(\"silver_erp_cust_az12\")\n",
    "print(\"Rows:\", spark.read.parquet(out_path).count())\n",
    "spark.sql(\"SELECT * FROM silver_erp_cust_az12 LIMIT 10\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834ccd64",
   "metadata": {},
   "source": [
    "### Table: `silver.erp_loc_a101`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb39238b",
   "metadata": {},
   "source": [
    "**Changes & Why**\n",
    "- Remove hyphens in `cid`; map country labels; add audit timestamp.\n",
    "\n",
    "**Columns (post‑transform):**\n",
    "\n",
    "| Column | Type | Nullable | Description |\n",
    "|---|---|---|---|\n",
    "| `cid` | `string` | Yes | Hyphens removed |\n",
    "| `cntry` | `string` | Yes | Mapped labels (DE/US) or n/a |\n",
    "| `dwh_create_date` | `timestamp` | Yes | Audit timestamp |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9053e8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "b = spark.read.parquet(f\"{GCS_BASE}/bronze/erp_loc_a101\")\n",
    "out = (b\n",
    "    .withColumn(\"cid\", F.regexp_replace(\"cid\",\"-\",\"\"))\n",
    "    .withColumn(\"cntry\",\n",
    "        F.when(F.trim(\"cntry\")==\"DE\",\"Germany\")\n",
    "         .when(F.trim(\"cntry\").isin(\"US\",\"USA\"),\"United States\")\n",
    "         .when((F.trim(\"cntry\")==\"\" ) | F.col(\"cntry\").isNull(), \"n/a\")\n",
    "         .otherwise(F.trim(\"cntry\"))\n",
    "    )\n",
    "    .withColumn(\"dwh_create_date\", F.current_timestamp())\n",
    ")\n",
    "out_path = f\"{GCS_BASE}/silver/erp_loc_a101\"\n",
    "out.write.mode(\"overwrite\").parquet(out_path)\n",
    "spark.read.parquet(out_path).createOrReplaceTempView(\"silver_erp_loc_a101\")\n",
    "print(\"Rows:\", spark.read.parquet(out_path).count())\n",
    "spark.sql(\"SELECT * FROM silver_erp_loc_a101 LIMIT 10\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668a881a",
   "metadata": {},
   "source": [
    "### Table: `silver.erp_px_cat_g1v2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178a938f",
   "metadata": {},
   "source": [
    "**Changes & Why**\n",
    "- Carry over mapping; add audit timestamp.\n",
    "\n",
    "**Columns (post‑transform):**\n",
    "\n",
    "| Column | Type | Nullable | Description |\n",
    "|---|---|---|---|\n",
    "| `id` | `string` | Yes | Category id |\n",
    "| `cat` | `string` | Yes | Category |\n",
    "| `subcat` | `string` | Yes | Subcategory |\n",
    "| `maintenance` | `string` | Yes | Maintenance attribute |\n",
    "| `dwh_create_date` | `timestamp` | Yes | Audit timestamp |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46d6502",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "b = spark.read.parquet(f\"{GCS_BASE}/bronze/erp_px_cat_g1v2\")\n",
    "out = b.withColumn(\"dwh_create_date\", F.current_timestamp())\n",
    "out_path = f\"{GCS_BASE}/silver/erp_px_cat_g1v2\"\n",
    "out.write.mode(\"overwrite\").parquet(out_path)\n",
    "spark.read.parquet(out_path).createOrReplaceTempView(\"silver_erp_px_cat_g1v2\")\n",
    "print(\"Rows:\", spark.read.parquet(out_path).count())\n",
    "spark.sql(\"SELECT * FROM silver_erp_px_cat_g1v2 LIMIT 10\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a32f97",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Wrote Silver tables as Parquet and registered temporary views."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
